{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install numpy tqdm scikit-learn tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 10:47:43.199224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733827663.218615  316865 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733827663.224547  316865 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 10:47:43.245387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import packages needed\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Lambda, Dense\n",
    "from keras import Model\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.ops import cast, maximum, square\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.ops import norm\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import Input\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (384, 384))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading all the image from the dataset folder\n",
    "def get_data():\n",
    "    # read all the folders\n",
    "    data_path = \"../../dataset/Musinsa_dataset\"\n",
    "    folders = os.listdir(data_path)\n",
    "\n",
    "    # read all the images inside the folders\n",
    "    style2index = []\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(len(folders)):\n",
    "        folder = folders[i]\n",
    "        folder_path = f\"{data_path}/{folder}\"\n",
    "\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        files = os.listdir(folder_path)\n",
    "        print(folder)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for file in tqdm(files):\n",
    "            try:\n",
    "                # read the image\n",
    "                image = load_image(f\"{folder_path}/{file}\")\n",
    "                image = image.astype(np.float16)\n",
    "\n",
    "                images.append(image)\n",
    "                labels.append(i)\n",
    "                count += 1\n",
    "                if count >= 100:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading image {file}: {e}\")\n",
    "        style2index.append({folder: i})\n",
    "\n",
    "    return images, labels, style2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 99/1642 [00:00<00:05, 293.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 99/1780 [00:00<00:05, 305.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cityboy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 99/2605 [00:00<00:09, 258.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 99/2870 [00:00<00:10, 253.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 99/1770 [00:00<00:06, 252.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preppy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 99/1355 [00:00<00:04, 252.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workwear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 99/2556 [00:00<00:09, 252.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 99/2670 [00:00<00:10, 244.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 99/1223 [00:00<00:04, 256.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gorpcore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 99/2631 [00:00<00:10, 246.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sporty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 99/2378 [00:00<00:08, 256.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romantic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 99/1795 [00:00<00:06, 256.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Girlish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 99/1783 [00:00<00:06, 255.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300 1300\n",
      "[{'Classic': 1}, {'Chic': 2}, {'Cityboy': 3}, {'Casual': 4}, {'Minimal': 5}, {'Preppy': 6}, {'Workwear': 7}, {'Retro': 8}, {'Street': 9}, {'Gorpcore': 10}, {'Sporty': 11}, {'Romantic': 12}, {'Girlish': 13}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image, labels, style2index = get_data()\n",
    "print(len(image), len(labels))\n",
    "print(style2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 1040, 1040\n",
      "test set size: 260, 260\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(image, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"train set size: {len(X_train)}, {len(y_train)}\")\n",
    "print(f\"test set size: {len(X_test)}, {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040, 384, 384, 3) (260, 384, 384, 3) (1040,) (260,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(y_train)\n",
    "Y_test = np.array(y_test)\n",
    "\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pair(X, y):\n",
    "    \"\"\"\n",
    "        For contrastive learning, we need the dataset in pair.\n",
    "        There should exist \n",
    "        Input: X(image), y(label)\n",
    "        Output: X_pairs(image pair), y_pairs(label pair)\n",
    "    \"\"\"\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    X_pairs = []\n",
    "    y_pairs = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        digit = y[i]\n",
    "\n",
    "        positive_digit_index = np.random.choice(np.where(y == digit)[0])\n",
    "        X_pairs.append([X[i], X[positive_digit_index]])\n",
    "        y_pairs.append([0])\n",
    "\n",
    "        negative_digit_index = np.random.choice(np.where(y!=digit)[0])\n",
    "        X_pairs.append([X[i], X[negative_digit_index]])\n",
    "        y_pairs.append([1])\n",
    "\n",
    "    indices = np.arange(len(X_pairs))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    return np.array(X_pairs)[indices], np.array(y_pairs)[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pairs shape:  (2080, 2, 384, 384, 3)\n",
      "X_test_pairs shape:  (520, 2, 384, 384, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_pairs, Y_train_pairs = generate_pair(X_train, y_train)\n",
    "X_test_pairs, Y_test_pairs = generate_pair(X_test, y_test)\n",
    "\n",
    "print(\"X_train_pairs shape: \", X_train_pairs.shape)\n",
    "print(\"X_test_pairs shape: \", X_test_pairs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet based model\n",
    "\n",
    "Using feature extraction part of pretrained ResNet model, one fc layer will be added on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1733827672.850424  316865 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22456 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:61:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Use backbone of pretrained model\n",
    "from keras.applications import EfficientNetV2S\n",
    "\n",
    "input1 = Input(shape=(384,384,3,))\n",
    "input2 = Input(shape=(384,384,3,))\n",
    "\n",
    "base_model = EfficientNetV2S(weights=\"imagenet\", include_top=True)\n",
    "\n",
    "network = Sequential(\n",
    "    [\n",
    "        Input(shape=(384, 384, 3)),\n",
    "        base_model,\n",
    "        Dense(256, activation=None)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "twin1 = network(input1)\n",
    "twin2 = network(input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(twins):\n",
    "    twin1_output, twin2_output = twins\n",
    "    twin1_norm = tf.linalg.l2_normalize(twin1_output, axis=1)\n",
    "    twin2_norm = tf.linalg.l2_normalize(twin2_output, axis=1)\n",
    "\n",
    "    cosine_similarity = twin1_norm * twin2_norm  # Element-wise multiplication\n",
    "    cosine_similarity = tf.reduce_sum(cosine_similarity, axis=1, keepdims=True)\n",
    "\n",
    "    return (1 - cosine_similarity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.ops import norm\n",
    "def euclidean_distance(twins):\n",
    "    \"\"\"Compute the euclidean distance (norm) of the output of\n",
    "    the twin networks.\n",
    "    \"\"\"\n",
    "    twin1_output, twin2_output = twins\n",
    "    return norm(twin1_output - twin2_output, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# distance = Lambda(euclidean_distance)([twin1, twin2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Lambda(cosine_distance)([twin1, twin2])\n",
    "# distance = Lambda(euclidean_distance)([twin1, twin2])\n",
    "model = Model(inputs=[input1, input2], outputs=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y, d):\n",
    "    \"\"\"\n",
    "    Compute the contrastive loss introduced by Yann LeCun et al. in the paper\n",
    "    \"Dimensionality Reduction by Learning an Invariant Mapping.\"\n",
    "    \"\"\"\n",
    "    margin = 1\n",
    "    y = cast(y, d.dtype)\n",
    "\n",
    "    loss = (1 - y) / 2 * square(d) + y / 2 * square(maximum(0.0, margin - d) + 1e-6)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.005)\n",
    "model.compile(loss=contrastive_loss, optimizer=optimizer, metrics=[binary_accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733827749.778618  317376 service.cc:148] XLA service 0x7fd768003610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1733827749.778753  317376 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2024-12-10 10:49:14.151146: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1733827762.892507  317376 cuda_dnn.cc:529] Loaded cuDNN version 90600\n",
      "I0000 00:00:1733827848.962485  317376 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 207ms/step - binary_accuracy: 0.5021 - loss: 0.1847 - val_binary_accuracy: 0.5000 - val_loss: 0.2500\n",
      "Epoch 2/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 145ms/step - binary_accuracy: 0.4835 - loss: 0.1813 - val_binary_accuracy: 0.4923 - val_loss: 0.2363\n",
      "Epoch 3/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 145ms/step - binary_accuracy: 0.4869 - loss: 0.1906 - val_binary_accuracy: 0.5000 - val_loss: 0.2500\n",
      "Epoch 4/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 139ms/step - binary_accuracy: 0.5061 - loss: 0.1978 - val_binary_accuracy: 0.5000 - val_loss: 0.1820\n",
      "Epoch 5/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 141ms/step - binary_accuracy: 0.5086 - loss: 0.1794 - val_binary_accuracy: 0.5077 - val_loss: 0.2011\n",
      "Epoch 6/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 140ms/step - binary_accuracy: 0.5034 - loss: 0.1723 - val_binary_accuracy: 0.4981 - val_loss: 0.1891\n",
      "Epoch 7/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 144ms/step - binary_accuracy: 0.5125 - loss: 0.1712 - val_binary_accuracy: 0.5096 - val_loss: 0.1659\n",
      "Epoch 8/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 138ms/step - binary_accuracy: 0.4922 - loss: 0.1678 - val_binary_accuracy: 0.5462 - val_loss: 0.1796\n",
      "Epoch 9/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 144ms/step - binary_accuracy: 0.4823 - loss: 0.1880 - val_binary_accuracy: 0.5250 - val_loss: 0.1734\n",
      "Epoch 10/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 141ms/step - binary_accuracy: 0.4947 - loss: 0.1796 - val_binary_accuracy: 0.4942 - val_loss: 0.1800\n",
      "Epoch 11/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 140ms/step - binary_accuracy: 0.5181 - loss: 0.1749 - val_binary_accuracy: 0.5250 - val_loss: 0.1710\n",
      "Epoch 12/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 138ms/step - binary_accuracy: 0.5078 - loss: 0.1765 - val_binary_accuracy: 0.5135 - val_loss: 0.1952\n",
      "Epoch 13/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 143ms/step - binary_accuracy: 0.4842 - loss: 0.1840 - val_binary_accuracy: 0.5000 - val_loss: 0.2500\n",
      "Epoch 14/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 137ms/step - binary_accuracy: 0.4870 - loss: 0.1749 - val_binary_accuracy: 0.5135 - val_loss: 0.1934\n",
      "Epoch 15/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 137ms/step - binary_accuracy: 0.5055 - loss: 0.1867 - val_binary_accuracy: 0.5000 - val_loss: 0.2265\n",
      "Epoch 16/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 137ms/step - binary_accuracy: 0.4884 - loss: 0.1968 - val_binary_accuracy: 0.5019 - val_loss: 0.2051\n",
      "Epoch 17/500\n",
      "\u001b[1m260/260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - binary_accuracy: 0.4962 - loss: 0.1989"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=100,         # Number of epochs with no improvement to stop training\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        x=[X_train_pairs[:, 0], X_train_pairs[:, 1]],\n",
    "        y=Y_train_pairs[:],\n",
    "        validation_data=([X_test_pairs[:, 0], X_test_pairs[:, 1]], Y_test_pairs[:]),\n",
    "        batch_size=8,\n",
    "        epochs=500,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([X_test_pairs[:, 0], X_test_pairs[:, 1]]) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers)\n",
    "print(model.layers[2].input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = model.layers[2]\n",
    "print(embedding_model)\n",
    "\n",
    "image_path = \"../../dataset/Musinsa_dataset/Cityboy/snap_card_1277506810595237743.jpg\"\n",
    "image = load_image(image_path)\n",
    "embedding = embedding_model.predict(image.reshape(1, 224, 224, 3))\n",
    "\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "embedding_model = Model(inputs=input1, outputs=twin1)\n",
    "\n",
    "\n",
    "embedding_model.save(\"embedding_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = load_model(\"embedding_model.h5\")\n",
    "\n",
    "image_path = \"../../dataset/Musinsa_dataset/Cityboy/snap_card_1277506810595237743.jpg\"\n",
    "image = load_image(image_path)\n",
    "\n",
    "embedding = loaded_model.predict(image.reshape(1, 224, 224, 3))\n",
    "\n",
    "print(\"Generated embedding: \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
